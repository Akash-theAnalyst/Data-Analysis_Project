# -*- coding: utf-8 -*-
"""DA Proj_CTR_RF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E11vaCvkCBUBr3k-J3-22nRcOcftcH4F
"""

from pandas import read_csv, get_dummies, Series, DataFrame, concat, crosstab,cut
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import GridSearchCV

#Data Reading
data1 = read_csv("/content/ad_records.csv")

#Data Validation
print(data1.head())
print(data1.describe())
print(data1.shape)
print(data1.info())
print(data1.isnull().sum())

#Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from pandas import to_datetime

#Device type and clicks based on them visual
plot = 'Gender'
plot_data = concat([data1['Clicked on Ad'], data1[plot]], axis =1) #Storing inside varible for plotting

#For Device plot
sns.countplot(x = "Gender", data = plot_data)

dev_typ_table = crosstab(index = data1["Gender"], columns = data1["Clicked on Ad"])

print("dev_typ_table")

#For Device and Click based on them plot
dev_typ_table.plot (kind = "bar", stacked = False)


# Gender and Clicked Ads Visualization
plot_data = concat([data1['Clicked on Ad'], data1['Gender']], axis=1)

sns.countplot(x='Gender', hue='Clicked on Ad', data=plot_data)
plt.title('Ad Clicks by Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

# Crosstab for numeric summary
gender_click_table = crosstab(index=data1["Gender"], columns=data1["Clicked on Ad"])
print("Gender vs Click Table:")
print(gender_click_table)

gender_click_table.plot(kind="bar", stacked=True)
plt.title('Ad Clicks Distribution by Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

# Age Group vs Clicked Ads
bins = [0, 25, 35, 50, 100]
labels = ['18–25', '26–35', '36–50', '50+']
data1['AgeGroup'] = cut(data1['Age'], bins=bins, labels=labels)

sns.countplot(x='AgeGroup', hue='Clicked on Ad', data=data1)
plt.title('Ad Clicks by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.show()

# CTR by Age Group
age_ctr = data1.groupby('AgeGroup', observed=True)['Clicked on Ad'].mean()
print(age_ctr)


# Converting Timestamp to datetime
data1['Timestamp'] = to_datetime(data1['Timestamp'])

# Extracting hour of day
data1['Hour'] = data1['Timestamp'].dt.hour

# Plotting clicks by hour
sns.countplot(x='Hour', hue='Clicked on Ad', data=data1)
plt.title('Ad Clicks by Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Count')
plt.show()

# Calculating CTR by hour
hour_ctr = data1.groupby('Hour', observed=True)['Clicked on Ad'].mean()
print(hour_ctr)

# Top 10 Ad Topic Lines by number of clicks
ad_ctr = data1.groupby('Ad Topic Line', observed=True)['Clicked on Ad'].mean().sort_values(ascending=False).head(10)
print(ad_ctr)

# Plotting top 10 Ad Topic Lines CTR
top_ads = ad_ctr.index
top_data = data1[data1['Ad Topic Line'].isin(top_ads)]

sns.barplot(x='Clicked on Ad', y='Ad Topic Line', data=top_data, estimator=lambda x: sum(x)/len(x))
plt.title('CTR by Top 10 Ad Topic Lines')
plt.xlabel('Click Through Rate')
plt.ylabel('Ad Topic Line')
plt.show()

# Extracting day of week from Timestamp
data1['DayOfWeek'] = data1['Timestamp'].dt.day_name()

# Plotting clicks by day of week
sns.countplot(x='DayOfWeek', hue='Clicked on Ad', data=data1,
              order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
plt.title('Ad Clicks by Day of Week')
plt.xlabel('Day of Week')
plt.ylabel('Count')
plt.show()

# Calculating CTR by day of week
day_ctr = data1.groupby('DayOfWeek', observed=True)['Clicked on Ad'].mean()
day_ctr = day_ctr[['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']]
print(day_ctr)

# Creating pivot table for heatmap: Hour vs Day of Week CTR
pivot = data1.pivot_table(values='Clicked on Ad', index=data1['Hour'], columns=data1['DayOfWeek'], aggfunc='mean')

# Reorder columns
pivot = pivot[['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']]

# Plotting heatmap
plt.figure(figsize=(12,6))
sns.heatmap(pivot, annot=True, fmt=".2f", cmap="YlGnBu")
plt.title("CTR Heatmap: Hour of Day vs Day of Week")
plt.xlabel("Day of Week")
plt.ylabel("Hour of Day")
plt.show()

# Keeping only numeric columns
numeric_cols = corr_data1.select_dtypes(include=['int64','float64'])

# Calculating correlation
rel = numeric_cols.corr(method='pearson')

# Plotting heatmap
f, ax = plt.subplots(figsize=(12,10))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(rel, cmap=cmap, vmax=.9, center=0, square=True,
            linewidth=0.5, cbar_kws={"shrink": 0.5}, annot=True)
plt.title("Correlation Heatmap (Numeric Features Only)")
plt.show()

# Data Mapping / Encoding to convert mappable objects to integers

print(data1['Gender'].unique())
print(data1['Clicked on Ad'].unique())

data1['Gender'] = data1['Gender'].map({"Male" :1, "Female" : 0})
print(data1['Gender'])
data1 = data1.drop('City', axis=1)

print(data1.info())

# Data Encoding
from sklearn.preprocessing import LabelEncoder

# Step 1: Convert Timestamp to datetime if not already
data1['Timestamp'] = to_datetime(data1['Timestamp'])

# Step 2: Extract Hour and DayOfWeek
data1['Hour'] = data1['Timestamp'].dt.hour
data1['DayOfWeek'] = data1['Timestamp'].dt.day_name()

# Step 3: Encode DayOfWeek and AgeGroup
le = LabelEncoder()
data1['DayOfWeek'] = le.fit_transform(data1['DayOfWeek'])
data1['AgeGroup'] = le.fit_transform(data1['AgeGroup'])  # make sure AgeGroup exists

# Step 4: Drop raw Timestamp
data1 = data1.drop('Timestamp', axis=1)

# Quick check
print(data1.head())
print(data1.info())

data2 = get_dummies(data1,['Ad Topic Line','Country'])

print(data2.describe())
print(data2.info())

#Feature Splitting, Data Scaling and Balancing

X = data2.drop('Clicked on Ad',axis=1) #Target Feature
Y = data2['Clicked on Ad'] #Labels

X_scaled = StandardScaler().fit_transform(X) #Scaling

X_train,X_test,Y_train,Y_test=train_test_split(X_scaled,Y,test_size=0.3, random_state=100) #Splitting
X_train,Y_train = SMOTE(random_state=100).fit_resample(X_train,Y_train) #Balancing

print(X_train)
print(Y_train)

# Random Forest Classifier (method 1)

RF_class1 = RandomForestClassifier(n_estimators=50, criterion='entropy', max_features='sqrt', random_state=1)  #Building model
RF_class1.fit(X_train,Y_train) #Training
Y_pred1 = RF_class1.predict(X_test) #Testing

#Evaluation - Accuracy and Confusion matrix

Accuracy=metrics.accuracy_score(Y_test, Y_pred1) #Calculating Accuracy
print("Accuracy :", Accuracy) #Verifying to see if this is a good metric??

con_matrix = metrics.confusion_matrix(Y_test, Y_pred1)
print(con_matrix)

recall = metrics.recall_score(Y_test, Y_pred1)
print('Recall =', recall)

precision=metrics.precision_score(Y_test, Y_pred1)
print('Precision =', precision)

f1=metrics.f1_score(Y_test, Y_pred1)
print('f1 =',f1)

#Accuracy : 0.851
#[[1323  215]
# [ 232 1230]]
#Recall = 0.841313269493844
#Precision = 0.8512110726643599
#f1 = 0.846233230134159

#Random Forest Classifier (method 2)

RF_class2 = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=1) #Building model
no_trees = {'n_estimators': [200, 250, 300, 350, 400, 450]}

Grid_search1 = GridSearchCV(estimator=RF_class2, param_grid=no_trees, scoring='recall', cv=5)
Grid_search1.fit(X_scaled, Y) #Training, Testing , Evaluation and Ranking

Best_parameters = Grid_search1.best_params_
print(Best_parameters)

Best_result = Grid_search1.best_score_
print(Best_result)

# {'n_estimators': 350}
# 0.8527557915457079

#Random Forest Classifier with best number of tree (method 1)

RF_class3 = RandomForestClassifier(n_estimators=350, criterion='entropy', max_features='sqrt', random_state=1) #Building model
RF_class3.fit(X_train,Y_train) #Training
Y_pred3 = RF_class3.predict(X_test) #Testing

Imp_features = Series(RF_class3.feature_importances_, index=list(X)).sort_values(ascending=False)
print(Imp_features)

#Age                                                     0.106567
#Daily Internet Usage                                    0.064271
#AgeGroup                                                0.061039
#Daily Time Spent on Site                                0.060560
#Area Income                                             0.060150

#Using Important features only (method 2)

X2 = data2[['Age', 'Daily Internet Usage', 'Daily Time Spent on Site', 'Area Income']]
X_scaled2 = StandardScaler().fit_transform(X2) #Scaling

RF_class4 = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=1) #Building Classifier
no_trees = {'n_estimators': [200, 250, 300, 350, 400, 450]}

Grid_search2 = GridSearchCV(estimator=RF_class4, param_grid=no_trees, scoring='recall', cv=5)
Grid_search2.fit(X_scaled2, Y) #Training, Testing , Evaluation and Ranking.

Best_parameters = Grid_search2.best_params_
print(Best_parameters)
Best_result = Grid_search2.best_score_
print(Best_result)

X2.head()

# {'n_estimators': 300}
# 0.7701839813413394

#Retraining Final Model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#Train-test split
X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X_scaled2, Y, test_size=0.3, random_state=100)

#Building final Random Forest with best n_estimators
RF_final2 = RandomForestClassifier(n_estimators=300, criterion='entropy', max_features='sqrt', random_state=1)
RF_final2.fit(X_train2, Y_train2)


Y_pred2 = RF_final2.predict(X_test2)

#Evaluation
print("Accuracy:", accuracy_score(Y_test2, Y_pred2))
print(classification_report(Y_test2, Y_pred2))

# Confusion matrix
cm2 = confusion_matrix(Y_test2, Y_pred2)
sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Top Feature Model")
plt.show()